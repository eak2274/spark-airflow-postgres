[2022-10-31T14:36:21.894+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: j_spark_trigger.transform_data_1 scheduled__2022-10-30T14:36:14.058284+00:00 [queued]>
[2022-10-31T14:36:21.953+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: j_spark_trigger.transform_data_1 scheduled__2022-10-30T14:36:14.058284+00:00 [queued]>
[2022-10-31T14:36:21.958+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-31T14:36:21.962+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 1
[2022-10-31T14:36:21.967+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-31T14:36:22.026+0000] {taskinstance.py:1383} INFO - Executing <Task(SparkSubmitOperator): transform_data_1> on 2022-10-30 14:36:14.058284+00:00
[2022-10-31T14:36:22.069+0000] {standard_task_runner.py:54} INFO - Started process 13880 to run task
[2022-10-31T14:36:22.088+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'j_spark_trigger', 'transform_data_1', 'scheduled__2022-10-30T14:36:14.058284+00:00', '--job-id', '122', '--raw', '--subdir', 'DAGS_FOLDER/j_spark_trigger.py', '--cfg-path', '/tmp/tmpndud3lc3']
[2022-10-31T14:36:22.094+0000] {standard_task_runner.py:83} INFO - Job 122: Subtask transform_data_1
[2022-10-31T14:36:22.111+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/j_spark_trigger.py
[2022-10-31T14:36:22.871+0000] {task_command.py:384} INFO - Running <TaskInstance: j_spark_trigger.transform_data_1 scheduled__2022-10-30T14:36:14.058284+00:00 [running]> on host 77a515ad4309
[2022-10-31T14:36:23.037+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=***@example.com
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=j_spark_trigger
AIRFLOW_CTX_TASK_ID=transform_data_1
AIRFLOW_CTX_EXECUTION_DATE=2022-10-30T14:36:14.058284+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-30T14:36:14.058284+00:00
[2022-10-31T14:36:23.072+0000] {spark_submit.py:204} INFO - Could not load connection string spark_default, defaulting to yarn
[2022-10-31T14:36:23.077+0000] {spark_submit.py:334} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /mnt/script-1.py
[2022-10-31T14:36:23.337+0000] {spark_submit.py:485} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2022-10-31T14:36:28.826+0000] {spark_submit.py:485} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2022-10-31T14:36:28.856+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:634)
[2022-10-31T14:36:28.866+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:274)
[2022-10-31T14:36:28.882+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:234)
[2022-10-31T14:36:28.888+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:119)
[2022-10-31T14:36:28.895+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1029)
[2022-10-31T14:36:28.906+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1029)
[2022-10-31T14:36:28.916+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)
[2022-10-31T14:36:28.923+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2022-10-31T14:36:28.932+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2022-10-31T14:36:28.937+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2022-10-31T14:36:29.762+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 417, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /mnt/script-1.py. Error code is: 1.
[2022-10-31T14:36:29.854+0000] {taskinstance.py:1406} INFO - Marking task as FAILED. dag_id=j_spark_trigger, task_id=transform_data_1, execution_date=20221030T143614, start_date=20221031T143621, end_date=20221031T143629
[2022-10-31T14:36:30.089+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 122 for task transform_data_1 (Cannot execute: spark-submit --master yarn --name arrow-spark /mnt/script-1.py. Error code is: 1.; 13880)
[2022-10-31T14:36:30.669+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-31T14:36:30.905+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
