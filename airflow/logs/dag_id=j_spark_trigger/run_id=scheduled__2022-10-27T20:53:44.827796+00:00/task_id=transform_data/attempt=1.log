[2022-10-28T20:53:47.764+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: j_spark_trigger.transform_data scheduled__2022-10-27T20:53:44.827796+00:00 [queued]>
[2022-10-28T20:53:47.786+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: j_spark_trigger.transform_data scheduled__2022-10-27T20:53:44.827796+00:00 [queued]>
[2022-10-28T20:53:47.790+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-28T20:53:47.793+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 2
[2022-10-28T20:53:47.795+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-28T20:53:47.816+0000] {taskinstance.py:1383} INFO - Executing <Task(SparkSubmitOperator): transform_data> on 2022-10-27 20:53:44.827796+00:00
[2022-10-28T20:53:47.831+0000] {standard_task_runner.py:54} INFO - Started process 3664 to run task
[2022-10-28T20:53:47.838+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'j_spark_trigger', 'transform_data', 'scheduled__2022-10-27T20:53:44.827796+00:00', '--job-id', '104', '--raw', '--subdir', 'DAGS_FOLDER/j_spark_trigger.py', '--cfg-path', '/tmp/tmp5uq99a1d']
[2022-10-28T20:53:47.864+0000] {standard_task_runner.py:83} INFO - Job 104: Subtask transform_data
[2022-10-28T20:53:47.878+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/j_spark_trigger.py
[2022-10-28T20:53:48.124+0000] {task_command.py:384} INFO - Running <TaskInstance: j_spark_trigger.transform_data scheduled__2022-10-27T20:53:44.827796+00:00 [running]> on host 08236190b7e3
[2022-10-28T20:53:48.269+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=***@example.com
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=j_spark_trigger
AIRFLOW_CTX_TASK_ID=transform_data
AIRFLOW_CTX_EXECUTION_DATE=2022-10-27T20:53:44.827796+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-27T20:53:44.827796+00:00
[2022-10-28T20:53:48.294+0000] {base.py:71} INFO - Using connection ID 'conn_spark' for task execution.
[2022-10-28T20:53:48.301+0000] {spark_submit.py:334} INFO - Spark-Submit cmd: spark-submit --master spark://py-spark:7077 --name arrow-spark --deploy-mode cluster /mnt/script-1.py
[2022-10-28T20:53:48.426+0000] {spark_submit.py:485} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2022-10-28T20:53:49.469+0000] {spark_submit.py:485} INFO - Exception in thread "main" org.apache.spark.SparkException: Cluster deploy mode is currently not supported for python applications on standalone clusters.
[2022-10-28T20:53:49.477+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:975)
[2022-10-28T20:53:49.479+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:273)
[2022-10-28T20:53:49.482+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)
[2022-10-28T20:53:49.485+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2022-10-28T20:53:49.487+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2022-10-28T20:53:49.488+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2022-10-28T20:53:49.490+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2022-10-28T20:53:49.493+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2022-10-28T20:53:49.494+0000] {spark_submit.py:485} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2022-10-28T20:53:49.548+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 417, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://py-spark:7077 --name arrow-spark --deploy-mode cluster /mnt/script-1.py. Error code is: 1.
[2022-10-28T20:53:49.558+0000] {taskinstance.py:1406} INFO - Marking task as UP_FOR_RETRY. dag_id=j_spark_trigger, task_id=transform_data, execution_date=20221027T205344, start_date=20221028T205347, end_date=20221028T205349
[2022-10-28T20:53:49.584+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 104 for task transform_data (Cannot execute: spark-submit --master spark://py-spark:7077 --name arrow-spark --deploy-mode cluster /mnt/script-1.py. Error code is: 1.; 3664)
[2022-10-28T20:53:49.683+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-28T20:53:49.743+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
